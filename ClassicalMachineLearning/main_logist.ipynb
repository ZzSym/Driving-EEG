{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "Logistic Regression is a supervised learning algorithm used for **binary and multi-class classification**. Unlike Linear Regression, which predicts continuous values, Logistic Regression predicts probabilities and uses a **sigmoid function** to map outputs to the range $[0,1]$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Methodology**\n",
    "\n",
    "### **1. Problem Definition**\n",
    "Suppose we have a dataset with $ N $ samples, where each sample $ \\mathbf{x}_j $ is a $ d $-dimensional vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_j = (x_{j1}, x_{j2}, \\dots, x_{jd})\n",
    "$$\n",
    "\n",
    "Each sample belongs to one of two classes $ y_j \\in \\{0,1\\} $. The goal of Logistic Regression is to model the probability that a given sample belongs to class 1:\n",
    "\n",
    "$$\n",
    "P(y=1 | \\mathbf{x}) = h(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "where $ h(\\mathbf{x}) $ is the hypothesis function.\n",
    "\n",
    "### **2. Sigmoid Function**\n",
    "The hypothesis function is modeled using a **sigmoid function**:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{w}^T\\mathbf{x} + b)}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\mathbf{w} $ is the weight vector.\n",
    "- $ b $ is the bias term.\n",
    "\n",
    "The sigmoid function ensures that the output is always between $ 0 $ and $ 1 $, making it interpretable as a probability.\n",
    "\n",
    "### **3. Cost Function (NLL)**\n",
    "The cost function for logistic regression is based on the **log-likelihood** of the data:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = -\\frac{1}{N} \\sum_{j=1}^{N} \\left[ y_j \\log h(\\mathbf{x}_j) + (1 - y_j) \\log (1 - h(\\mathbf{x}_j)) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### **4. Optimization using Gradient Descent**\n",
    "To minimize the cost function, we use **gradient descent**:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{w}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "where $ \\alpha $ is the learning rate. The gradients are computed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{N} \\sum_{j=1}^{N} (h(\\mathbf{x}_j) - y_j) \\mathbf{x}_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{N} \\sum_{j=1}^{N} (h(\\mathbf{x}_j) - y_j)\n",
    "$$\n",
    "\n",
    "### **5. Decision Rule**\n",
    "After training, predictions are made using:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } h(\\mathbf{x}) \\geq 0.5 \\\\\n",
    "0, & \\text{if } h(\\mathbf{x}) < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For multi-class classification, we extend Logistic Regression using **Softmax Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "- `numpy`\n",
    "- `scikit-learn`\n",
    "- `matplotlib`\n",
    "\n",
    "---\n",
    "\n",
    "The following is an example of using Logistic Regression to analyze EEG data recorded in a driving simulation experiment. The goal is to classify **steering angles** based on EEG signals.\n",
    "\n",
    "You can try applying Logistic Regression to other BCI (Brain-Computer Interface) datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import (\n",
    "    load_npz_files,\n",
    "    z_score_norm,\n",
    "    session_wise_grouping,\n",
    "    make_binary_label,\n",
    "    make_multi_label,\n",
    "    load_data_model,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test data\n",
    "data_dir = \"../data/data_model/sub-jimingda/forward1.npz\"\n",
    "data_list = load_data_model(data_dir)\n",
    "train_data, train_label, test_data, test_label = data_list[\"feature_train\"], data_list[\"steering_train\"], data_list[\"feature_test\"], data_list[\"steering_test\"]\n",
    "\n",
    "# simply flatten the data from (sample, channel, timepoint) to (sample, channel*timepoint)\n",
    "train_data = np.reshape(train_data, (train_data.shape[0], -1))\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], -1))\n",
    "\n",
    "# make binary labels\n",
    "# train_label = make_binary_label(train_label)\n",
    "# test_label = make_binary_label(test_label)\n",
    "\n",
    "# make multi labels\n",
    "split_list = [-1, -0.1, 0.1, 1]\n",
    "train_label = make_multi_label(train_label, split_list)\n",
    "test_label = make_multi_label(test_label, split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.53%\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(train_data, train_label)\n",
    "# predict\n",
    "pred = model.predict(test_data)\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(test_label, pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
